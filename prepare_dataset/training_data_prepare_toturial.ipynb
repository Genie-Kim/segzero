{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1. Initialize sam2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# if using Apple MPS, fall back to CPU for unsupported ops\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import numpy as np\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# select the device for computation\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "if device.type == \"cuda\":\n",
    "    # use bfloat16 for the entire notebook\n",
    "    torch.autocast(\"cuda\", dtype=torch.bfloat16).__enter__()\n",
    "    # turn on tfloat32 for Ampere GPUs (https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices)\n",
    "    if torch.cuda.get_device_properties(0).major >= 8:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "        torch.backends.cudnn.allow_tf32 = True\n",
    "elif device.type == \"mps\":\n",
    "    print(\n",
    "        \"\\nSupport for MPS devices is preliminary. SAM 2 is trained with CUDA and might \"\n",
    "        \"give numerically different outputs and sometimes degraded performance on MPS. \"\n",
    "        \"See e.g. https://github.com/pytorch/pytorch/issues/84936 for a discussion.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "def show_mask(mask, ax, random_color=False, borders = True):\n",
    "    if random_color:\n",
    "        color = np.concatenate([np.random.random(3), np.array([0.6])], axis=0)\n",
    "    else:\n",
    "        color = np.array([30/255, 144/255, 255/255, 0.6])\n",
    "    h, w = mask.shape[-2:]\n",
    "    mask = mask.astype(np.uint8)\n",
    "    mask_image =  mask.reshape(h, w, 1) * color.reshape(1, 1, -1)\n",
    "    if borders:\n",
    "        import cv2\n",
    "        contours, _ = cv2.findContours(mask,cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE) \n",
    "        # Try to smooth contours\n",
    "        contours = [cv2.approxPolyDP(contour, epsilon=0.01, closed=True) for contour in contours]\n",
    "        mask_image = cv2.drawContours(mask_image, contours, -1, (1, 1, 1, 0.5), thickness=2) \n",
    "    ax.imshow(mask_image)\n",
    "\n",
    "def show_points(coords, labels, ax, marker_size=375):\n",
    "    pos_points = coords[labels==1]\n",
    "    neg_points = coords[labels==0]\n",
    "    ax.scatter(pos_points[:, 0], pos_points[:, 1], color='green', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)\n",
    "    ax.scatter(neg_points[:, 0], neg_points[:, 1], color='red', marker='*', s=marker_size, edgecolor='white', linewidth=1.25)   \n",
    "\n",
    "def show_box(box, ax):\n",
    "    x0, y0 = box[0], box[1]\n",
    "    w, h = box[2] - box[0], box[3] - box[1]\n",
    "    ax.add_patch(plt.Rectangle((x0, y0), w, h, edgecolor='green', facecolor=(0, 0, 0, 0), lw=2))    \n",
    "\n",
    "def show_masks(image, masks, scores, point_coords=None, box_coords=None, input_labels=None, borders=True):\n",
    "    for i, (mask, score) in enumerate(zip(masks, scores)):\n",
    "        plt.figure(figsize=(10, 10))\n",
    "        plt.imshow(image)\n",
    "        show_mask(mask, plt.gca(), borders=borders)\n",
    "        if point_coords is not None:\n",
    "            assert input_labels is not None\n",
    "            show_points(point_coords, input_labels, plt.gca())\n",
    "        if box_coords is not None:\n",
    "            # boxes\n",
    "            show_box(box_coords, plt.gca())\n",
    "        if len(scores) > 1:\n",
    "            plt.title(f\"Mask {i+1}, Score: {score:.3f}\", fontsize=18)\n",
    "        plt.axis('off')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sam2.build_sam import build_sam2\n",
    "from sam2.sam2_image_predictor import SAM2ImagePredictor\n",
    "\n",
    "sam2_checkpoint = \"pretrained_models/sam2.1_hiera_large.pt\" # input your own model path\n",
    "model_cfg = \"configs/sam2.1/sam2.1_hiera_l.yaml\"\n",
    "\n",
    "sam2_model = build_sam2(model_cfg, sam2_checkpoint, device=device)\n",
    "\n",
    "predictor = SAM2ImagePredictor(sam2_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Load reason seg data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "import glob\n",
    "\n",
    "data_dir = \"data/Segmentation/reason_seg/train\" # input your own train data path\n",
    "\n",
    "json_path_list = sorted(glob.glob(data_dir + \"/*.json\"))\n",
    "json_path_list[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "annotation = json.load(open(json_path_list[0], 'r'))\n",
    "texts = annotation[\"text\"]\n",
    "print(texts)\n",
    "print(len(texts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import cv2\n",
    "\n",
    "def get_mask_from_json(json_path, height, width):\n",
    "    try:\n",
    "        with open(json_path, \"r\") as r:\n",
    "            anno = json.loads(r.read())\n",
    "    except:\n",
    "        with open(json_path, \"r\", encoding=\"cp1252\") as r:\n",
    "            anno = json.loads(r.read())\n",
    "\n",
    "    inform = anno[\"shapes\"]\n",
    "\n",
    "    ### sort polies by area\n",
    "    area_list = []\n",
    "    valid_poly_list = []\n",
    "    for i in inform:\n",
    "        label_id = i[\"label\"]\n",
    "        points = i[\"points\"]\n",
    "        if \"flag\" == label_id.lower():  ## meaningless deprecated annotations\n",
    "            continue\n",
    "\n",
    "        tmp_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "        cv2.polylines(tmp_mask, np.array([points], dtype=np.int32), True, 1, 1)\n",
    "        cv2.fillPoly(tmp_mask, np.array([points], dtype=np.int32), 1)\n",
    "        tmp_area = tmp_mask.sum()\n",
    "\n",
    "        area_list.append(tmp_area)\n",
    "        valid_poly_list.append(i)\n",
    "\n",
    "    ### ground-truth mask\n",
    "    sort_index = np.argsort(area_list)[::-1].astype(np.int32)\n",
    "    sort_index = list(sort_index)\n",
    "    sort_inform = []\n",
    "    for s_idx in sort_index:\n",
    "        sort_inform.append(valid_poly_list[s_idx])\n",
    "\n",
    "    mask = np.zeros((height, width), dtype=np.uint8)\n",
    "    for i in sort_inform:\n",
    "        label_id = i[\"label\"]\n",
    "        points = i[\"points\"]\n",
    "\n",
    "        if \"ignore\" in label_id.lower():\n",
    "            label_value = 255  # ignored during evaluation\n",
    "        else:\n",
    "            label_value = 1  # target\n",
    "\n",
    "        cv2.polylines(mask, np.array([points], dtype=np.int32), True, label_value, 1)\n",
    "        cv2.fillPoly(mask, np.array([points], dtype=np.int32), label_value)\n",
    "\n",
    "    mask = mask.astype(bool)  \n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import ndimage\n",
    "def get_two_representative_points(m):\n",
    "    \"\"\"\n",
    "    找到两个能较好描述mask形状的点\n",
    "    \n",
    "    Args:\n",
    "        m: 二值图像数组\n",
    "    \n",
    "    Returns:\n",
    "        tuple: ((x1, y1), (x2, y2)) 两个代表性点的坐标\n",
    "    \"\"\"\n",
    "    y_indices, x_indices = np.where(m == 1)\n",
    "    if len(x_indices) == 0 or len(y_indices) == 0:\n",
    "        return None, None\n",
    "    \n",
    "    # 计算距离变换\n",
    "    dist_transform = ndimage.distance_transform_edt(m)\n",
    "    \n",
    "    # 找到第一个点（全局最大值点）\n",
    "    y1, x1 = np.unravel_index(dist_transform.argmax(), dist_transform.shape)\n",
    "    \n",
    "    # 计算mask的重心\n",
    "    center_y = int(np.mean(y_indices))\n",
    "    center_x = int(np.mean(x_indices))\n",
    "    \n",
    "    # 将点分为两组：距离第一个点较远的点和较近的点\n",
    "    points = np.column_stack((y_indices, x_indices))\n",
    "    distances_to_first = ((points[:, 0] - y1) ** 2 + (points[:, 1] - x1) ** 2) ** 0.5\n",
    "    \n",
    "    # 找到距离第一个点最远的点集\n",
    "    far_points = points[distances_to_first > np.median(distances_to_first)]\n",
    "    \n",
    "    if len(far_points) > 0:\n",
    "        # 在远点中找到距离变换值最大的点作为第二个点\n",
    "        far_dist_values = dist_transform[far_points[:, 0], far_points[:, 1]]\n",
    "        second_point_idx = np.argmax(far_dist_values)\n",
    "        y2, x2 = far_points[second_point_idx]\n",
    "    else:\n",
    "        # 如果没有合适的远点，使用重心附近的点\n",
    "        local_region = dist_transform[\n",
    "            max(0, center_y - 10):min(m.shape[0], center_y + 10),\n",
    "            max(0, center_x - 10):min(m.shape[1], center_x + 10)\n",
    "        ]\n",
    "        local_y, local_x = np.unravel_index(local_region.argmax(), local_region.shape)\n",
    "        y2 = local_y + max(0, center_y - 10)\n",
    "        x2 = local_x + max(0, center_x - 10)\n",
    "    \n",
    "    # 确保两个点都在mask上\n",
    "    if m[y1, x1] == 0:\n",
    "        distances = (x_indices - x1)**2 + (y_indices - y1)**2\n",
    "        nearest_idx = np.argmin(distances)\n",
    "        x1, y1 = int(x_indices[nearest_idx]), int(y_indices[nearest_idx])\n",
    "    \n",
    "    if m[y2, x2] == 0:\n",
    "        distances = (x_indices - x2)**2 + (y_indices - y2)**2\n",
    "        nearest_idx = np.argmin(distances)\n",
    "        x2, y2 = int(x_indices[nearest_idx]), int(y_indices[nearest_idx])\n",
    "    \n",
    "    return [x1, y1], [x2, y2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mask_from_point(predictor, input_point, input_label, box):\n",
    "    masks, scores, logits = predictor.predict(\n",
    "        point_coords=input_point,\n",
    "        point_labels=input_label,\n",
    "        box=box,\n",
    "        multimask_output=False,\n",
    "    )\n",
    "    sorted_ind = np.argsort(scores)[::-1]\n",
    "    masks = masks[sorted_ind]\n",
    "    scores = scores[sorted_ind]\n",
    "    logits = logits[sorted_ind]\n",
    "    return masks\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def compute_iou(mask1, mask2):\n",
    "    intersection = np.logical_and(mask1, mask2).sum()\n",
    "    union = np.logical_or(mask1, mask2).sum()\n",
    "    if union == 0:\n",
    "        return 0\n",
    "    return intersection / union\n",
    "\n",
    "def is_bbox_contained(inner_bbox, outer_bbox):\n",
    "    \"\"\"\n",
    "    判断bbox1是否完全包含在bbox2中\n",
    "    bbox格式: [x1, y1, x2, y2]\n",
    "    \"\"\"\n",
    "    return (inner_bbox[0] >= outer_bbox[0] and  # bbox1的左边界在bbox2的左边界的右边\n",
    "            inner_bbox[1] >= outer_bbox[1] and  # bbox1的上边界在bbox2的上边界的下边\n",
    "            inner_bbox[2] <= outer_bbox[2] and  # bbox1的右边界在bbox2的右边界的左边\n",
    "            inner_bbox[3] <= outer_bbox[3])     # bbox1的下边界在bbox2的下边界的上边"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Generate annotation list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm  # 导入tqdm\n",
    "import json  # 导入json模块\n",
    "import cv2\n",
    "\n",
    "threshold_iou = 0.6  # threshold_iou IOU:  0.659445961\n",
    "cnt = 0\n",
    "\n",
    "seg_zero_annotation_list = []\n",
    "\n",
    "for idx, json_path in tqdm(enumerate(json_path_list), desc=\"Processing images\"):  # 使用tqdm包装循环\n",
    "    image_path = json_path.replace(\".json\", \".jpg\")\n",
    "    image_id = image_path.split(\"/\")[-1].split(\".\")[0]\n",
    "    \n",
    "    \n",
    "    anno = json.loads(open(json_path, \"r\").read())\n",
    "    text = anno[\"text\"][0]\n",
    "\n",
    "    \n",
    "    # # set Image to SAM2\n",
    "    # image = Image.open(image_path)\n",
    "    # image = np.array(image.convert(\"RGB\"))\n",
    "    image = cv2.imread(image_path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    height, width, _ = image.shape\n",
    "    predictor.set_image(image)\n",
    "\n",
    "    \n",
    "    inform = anno[\"shapes\"]\n",
    "\n",
    "    ### sort polies by area\n",
    "    area_list = []\n",
    "    valid_poly_list = []\n",
    "    for i in inform:\n",
    "        label_id = i[\"label\"]\n",
    "        points = i[\"points\"]\n",
    "        if \"flag\" == label_id.lower():  ## meaningless deprecated annotations\n",
    "            continue\n",
    "\n",
    "        tmp_mask = np.zeros((height, width), dtype=np.uint8)\n",
    "        cv2.polylines(tmp_mask, np.array([points], dtype=np.int32), True, 1, 1)\n",
    "        cv2.fillPoly(tmp_mask, np.array([points], dtype=np.int32), 1)\n",
    "        tmp_area = tmp_mask.sum()\n",
    "\n",
    "        area_list.append(tmp_area)\n",
    "        valid_poly_list.append(i)\n",
    "\n",
    "    ### ground-truth mask\n",
    "    sort_index = np.argsort(area_list)[::-1].astype(np.int32)\n",
    "    sort_index = list(sort_index)\n",
    "    sort_inform = []\n",
    "    for s_idx in sort_index:\n",
    "        sort_inform.append(valid_poly_list[s_idx])\n",
    "\n",
    "    \n",
    "    bboxes_list = []\n",
    "    points_list = []\n",
    "    prev_bbox = None\n",
    "    for i in sort_inform:\n",
    "        m = np.zeros((height, width), dtype=np.uint8)\n",
    "        label_id = i[\"label\"]\n",
    "        points = i[\"points\"]\n",
    "\n",
    "        if \"ignore\" in label_id.lower():\n",
    "            label_value = 255  # ignored during evaluation\n",
    "        else:\n",
    "            label_value = 1  # target\n",
    "\n",
    "        cv2.polylines(m, np.array([points], dtype=np.int32), True, label_value, 1)\n",
    "        cv2.fillPoly(m, np.array([points], dtype=np.int32), label_value)\n",
    "\n",
    "        m = m.astype(bool).astype(np.uint8)  \n",
    "        # plt.imshow(m)\n",
    "        # plt.show()\n",
    "        left = np.where(m == 1)[1].min()\n",
    "        top = np.where(m == 1)[0].min()\n",
    "        right = np.where(m == 1)[1].max()\n",
    "        bottom = np.where(m == 1)[0].max()\n",
    "        box = [left, top, right, bottom]\n",
    "        # print(box)\n",
    "        if prev_bbox is not None:\n",
    "            if is_bbox_contained(box, prev_bbox):\n",
    "                continue\n",
    "            else:\n",
    "                prev_bbox = box\n",
    "        points_1, points_2 = get_two_representative_points(m)\n",
    "        point = points_1\n",
    "        label = 1\n",
    "        \n",
    "        mask_pred = get_mask_from_point(predictor, np.array([point]), np.array([label]), np.array(box))\n",
    "\n",
    "        mask_pred = mask_pred[0].astype(bool)\n",
    "        mask_gt = m.astype(bool)\n",
    "        iou = compute_iou(mask_pred, mask_gt)\n",
    "        \n",
    "        if iou < threshold_iou:\n",
    "            continue\n",
    "        \n",
    "        \n",
    "        bboxes_list.append(box)\n",
    "        points_list.append(point)\n",
    "\n",
    "    if len(bboxes_list) <= 0:\n",
    "        continue\n",
    "    \n",
    "    seg_zero_annotation_list.append({\n",
    "        \"id\": \"reason_seg_\" + image_id,\n",
    "        \"image_id\": image_id,\n",
    "        \"image_path\": image_path,\n",
    "        \"problem\": text,\n",
    "        \"bboxes\": bboxes_list,\n",
    "        \"center_points\": points_list\n",
    "    })\n",
    "        \n",
    "    cnt += 1\n",
    "        \n",
    "    # if cnt > 10:\n",
    "    #     break\n",
    "        \n",
    "            \n",
    "print(f\"总共发现 {len(seg_zero_annotation_list)} 个高于阈值IOU的案例\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_zero_annotation_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in seg_zero_annotation_list:\n",
    "    item['bboxes'] = [list(map(int, bbox)) for bbox in item['bboxes']]\n",
    "    item['center_points'] = [list(map(int, center_point)) for center_point in item['center_points']]\n",
    "seg_zero_annotation_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Save and show examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'seg_zero_reasonseg_annotation_list_all_various_item.json', 'w', encoding='utf-8') as f:\n",
    "    json.dump(seg_zero_annotation_list, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 \n",
    "\n",
    "item = seg_zero_annotation_list[30]\n",
    "\n",
    "print(item['problem'])\n",
    "print(item['bboxes'])\n",
    "print(item['center_points'])\n",
    "\n",
    "image_path = item['image_path']\n",
    "image = cv2.imread(image_path)\n",
    "image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "\n",
    "for bbox, center_point in zip(item['bboxes'], item['center_points']):\n",
    "    cv2.rectangle(image, (bbox[0], bbox[1]), (bbox[2], bbox[3]), (0, 0, 255), 2)\n",
    "    cv2.circle(image, (center_point[0], center_point[1]), 5, (0, 255, 0), -1)\n",
    "    \n",
    "plt.imshow(image)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Please refer to gen_training_dataset.py"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seg_zero",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
